# S3 API (Python)

## Required Packages

- pandas
- geopandas
- boto3
- s3f3

## Importing

1. Create `project_root/src/s3_scripts/` directory and ensure `s3_api.py` is in that directory

2. Create `project_root/src/s3_scripts/s3_secret.py` containing the following code:

```python
s3_access_key_id = "key ID"
s3_secret_access_key = "secret key"
```

3. In python script using the api add the following code to your imports:

```python
sys.path.append(os.path.join("dir_to_src_folder", "s3_scripts"))
from s3_api import s3_api
```

## Functions

- The api can be initialized as follows:

```
api = s3_api()
```

- `set_root(dir)` sets a value "root" in the api so it knows the root project directory. The default root value is the working directory of the file. This should be used when working with shapefiles to meet the assumptions made about the project folder structure. Example:

```
api = s3_api()
api.set_root("C:\\..\\my_project_dir")
```

- The api holds a [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html) client object. The pre-initialized client can be accessed using `get_client()` or a new client can be created using `get_new_client()`.

- `list_buckets()` returns all the buckets available in the form:

```
[{'Name': 'bucket_name1', 'CreationDate': datetime.datetime()}, {'Name': 'bucket_name2', 'CreationDate': datetime.datetime()}, ...]
```

- `list_bucket_keys()` returns a list of all the recursive paths in the bucket in the form:

```python
["root_folder", "root_folder/folder1", "root_folder/folder2", "root_folder/folder1/sub_folder1", ...]
```

- `get_bucket_df(key)` takes in a key (s3 path to file) and returns a pandas dataframe of the data. _Note_: this uses the boto3 package.

```python
my_df = api.get_bucket_df("s3_path/to/data/my_df.csv")
```

- `get_bucket_geoDF(key, rb = True)` takes in a key and returns a geopandas dataframe. This should be used over `get_bucket_df(key)` when working with raster data. By default, it will read the file with the **rb** option. If set to false, it will read using the **r** option. _Note_: This uses the s3fs package.

```python
my_gdf = api.get_bucket_geoDF("s3_path/to/data/my_geo_df.geojson")
```

- `get_bucket_shp(remote_dir, local_dir)` takes in a key and returns a geopandas dataframe. This should be used over `get_bucket_df(key)` and `get_bucket_geoDF(key, rb)` when working with specifically shapefiles. Since multiple files are needed at the same time for geopandas to read the shapefile correctly, this function downloads all the files in a given remote directory (s3 key) to the specified local folder. This folder should already exist before running this function. _Note_: This uses the boto3 package.

  - **_Warning:_** This function requires the root to be accurately set or the data will be stored in an unexpected location. Also, if the give local directory has the same named files as the remote files, the local files will be overwritten.

```python
my_gdf = api.get_bucket_shp(remote_dir = "s3_path/to/shapefiles/",
                            local_dir = os.path.join(proj_root, "data", "shapefiles"))
```

- `write_csv(df, file_path, file_name)` takes in a pandas dataframe, file path of the folder that contains the data, and the file name for the data. This will write a pandas dataframe into a csv onto the s3 bucket. Example:

```python
api.write_csv(my_df, "path/to/data/folder", "my_df.csv")
```

- `write_GeoJSON(gdf, file_path, file_name)` takes in a geopandas dataframe, file path of the folder that contains the data, and the file name for the data. This will write a geopandas dataframe to a file using the GeoJSON driver onto the s3 bucket. Example:

```python
api.write_GeoJSON(my_geo_df, "path/to/data/folder", "my_geo_df.geojson")
```

- `write_shp(gdf, file_path, file_name)` takes in a geopandas dataframe, file path of the folder that contains the data, and the file name for the data. This will write a geopandas dataframe to a file using the ESRI driver onto a local directory. The script assumes the file is in the `project_root/src/s3_scripts` directory. The script will then create a temp directory at `project_root/data/temp/file_name/`. Lastly it will upload each file associated with the shapefile, including the shapefile, into the s3 bucket at the folder given by the _file_path_ parameter.
  - **_Warning:_** This function requires the root to be accurately set or the data will be stored in an unexpected location.

```python
api.write_shp(my_geo_df, "path/to/data/folder", "my_geo_df.shp")
```
